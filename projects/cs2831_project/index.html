<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Semantic Segmentation of Aerial Photographs | Grayson M. Martin </title> <meta name="author" content="Grayson M. Martin"> <meta name="description" content="Per-pixel land use classification of sattelite imagery of Mumbai. Completed for CS2831 - Advanced Computer Vision"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://graysonmmartin.github.io/projects/cs2831_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Grayson</span> M. Martin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Semantic Segmentation of Aerial Photographs</h1> <p class="post-description">Per-pixel land use classification of sattelite imagery of Mumbai. Completed for CS2831 - Advanced Computer Vision</p> </header> <article> <h2 id="abstract">Abstract</h2> <p>Semantic segmentation of aerial imagery is a critical tool for applications such as environmental monitoring, urban planning, and disaster assessment. In this project, I employed the U-Net architecture attempting a variety of enhancements to improve segmentation accuracy on a set of satellite images of Mumbai. Seven models were trained and evaluated, each integrating specific changes to the base model in loss function, encoder depth, dropout regularization, and addition of attention mechanisms. Metrics including IoU, Dice, Precision, and Recall were used to assess model performance across six classes: vegetation, built-up areas, informal settlements, impervious surfaces, barren land, and water. A small “unclassified” class is also considered.</p> <h2 id="introduction">Introduction</h2> <p>Aerial photography has a wide variety of uses, including geology, archaeology, disaster assessment, and environmental monitoring <a class="citation" href="#NASM_AerialPhotography">(National Air and Space Museum, 2023)</a>. The technology improved and became more widely utilized for military purposes starting in World War I, and has since gone on to be used for tasks such as identifying different vegetation types, detecting diseased and damaged vegetation, and counting how many missiles, planes, and other military hardware adversaries have and where it is located <a class="citation" href="#Baumann_RemoteSensingHistory">(Baumann, 2014)</a>. Semantic segmentation, or pixel-wise classification, is useful in this context. Creating a mask that classifies all regions of an aerial image allows for monitoring of environmental conditions, foreign objects, and changing conditions over time. While this project explores semantic segmentation of aerial images, the technology is useful for a broad number of tasks in biology, robotics, agriculture, sports analysis, and more.</p> <h3 id="semantic-segmentation-performance-metrics">Semantic Segmentation Performance Metrics</h3> <p>Performance metrics for semantic segmentation can be thought of in terms of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) classifications for each pixel.</p> <p>The most common performance metric for semantic segmentation is the Jaccard Score, also known as <em>Intersection over Union (IoU)</em>.</p> \[\mathrm{IoU} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP} + \mathrm{FN}}\] <p>Another frequently used metric for semantic segmentation is the Dice Score, also known as the <em>F1 Score</em>. This metric is formulated from two related metrics, <em>Precision</em> and <em>Recall</em>.</p> \[\mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}, \quad \mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}\] \[\mathrm{F1} = 2 \times \frac{\mathrm{Precision} \times \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}\] <p>Intuitively, Precision can be interpreted as the proportion of predicted positive pixels that are correctly segmented, Recall as the proportion of ground truth pixels that are correctly segmented, and F1 as the harmonic mean of both.</p> <h3 id="transposed-convolution">Transposed Convolution</h3> <p>Transposed convolution, also known as <em>upconvolution</em>, is a method of upsampling similar to downsampling with convolution. Between each input pixel, zeros are inserted to increase the size of the feature map before convolution with the kernel. An example of transposed convolution is given below:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/didl_transposed_convolution-480.webp 480w,/assets/img/didl_transposed_convolution-800.webp 800w,/assets/img/didl_transposed_convolution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/didl_transposed_convolution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="An Example Upconvolution with a 2×2 Input, 2×2 Kernel, and Stride of 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An Example Upconvolution with a 2×2 Input, 2×2 Kernel, and Stride of 1 <a class="citation" href="#zhang2023dive">(Zhang et al., 2023)</a> </div> <h2 id="dataset-description">Dataset Description</h2> <p>The dataset explored in this project is the Manually Annotated High Resolution Satellite Image Dataset of Mumbai for Semantic Segmentation (missing reference). The dataset was created from high-resolution, true-color satellite imagery of Pleiades-1A acquired on March 15, 2017 over Mumbai. There are six classifications: vegetation, built-up areas, informal settlements, impervious surfaces (roads, streets, parking lots, etc.), barren land, water, and a small number of unclassified pixels. The exact pixel distribution for the training set is given in Table 1.</p> <table> <thead> <tr> <th>Class</th> <th style="text-align: right">Informal Settlements</th> <th style="text-align: right">Built-Up</th> <th style="text-align: right">Impervious Surfaces</th> <th style="text-align: right">Vegetation</th> <th style="text-align: right">Barren</th> <th style="text-align: right">Water</th> <th style="text-align: right">Unclassified</th> </tr> </thead> <tbody> <tr> <td><strong># pixels</strong></td> <td style="text-align: right">12,921,604</td> <td style="text-align: right">11,358,632</td> <td style="text-align: right">13,436,578</td> <td style="text-align: right">22,423,411</td> <td style="text-align: right">18,735,038</td> <td style="text-align: right">37,789,523</td> <td style="text-align: right">120,366</td> </tr> </tbody> </table> <p><em>Table 1: Training Patch Pixel Distribution</em></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Dabra_class_balance-480.webp 480w,/assets/img/Dabra_class_balance-800.webp 800w,/assets/img/Dabra_class_balance-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Dabra_class_balance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Distribution of Labels Across Patches" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Distribution of Labels Across Patches <a class="citation" href="#Dabra2023">(Dabra &amp; Kumar, 2023)</a> </div> <h2 id="methods">Methods</h2> <h3 id="u-net-architecture">U-Net Architecture</h3> <p>Ronneberger, Fischer, and Brox proposed the first U-Net architecture to segment cells in microscopic images <a class="citation" href="#ronneberger2015unetconvolutionalnetworksbiomedical">(Ronneberger et al., 2015)</a>. The symmetric model consists of a contracting path, or encoder, on the left half and an expanding path, or decoder, on the right half. In the encoder, each level applies a $3\times 3$ convolution with ReLU activation before a $2\times 2$ max pooling operation is applied with stride 2 which reduces spatial dimensions by half. By doing this, the encoder is learning increasingly abstract representations of the original image while downsampling for computational efficiency. The decoder starts with the lowest spatial resolution, most abstract features and performs a series of $2\times 2$ upconvolutions which doubles the spatial dimensions. Importantly, a skip connection from the encoder of equivalent spatial dimension to each decoder layer is included to preserve spatial information lost during downsampling. The final output layer performs a $1\times 1$ convolution that reduces the number of channels to the number of classes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Ronneberger_architecture-480.webp 480w,/assets/img/Ronneberger_architecture-800.webp 800w,/assets/img/Ronneberger_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Ronneberger_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="The Original U-Net Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Original U-Net Architecture <a class="citation" href="#ronneberger2015unetconvolutionalnetworksbiomedical">(Ronneberger et al., 2015)</a> </div> <h3 id="loss-functions">Loss Functions</h3> <p>As with most classification networks, the output of our semantic segmentation model at each pixel is a vector of probabilities <strong>p</strong>, where each probability for class <em>k</em> is given by the softmax of the activations of each class input:</p> \[p_k(x, y) = \frac{\exp\bigl(a_k(x, y)\bigr)}{\sum_{k'=1}^{|k|} \exp\bigl(a_{k'}(x, y)\bigr)}\] <p>where $a_k(x,y)$ is the activation of class $k$ at pixel $(x,y)$ and $\lvert\mathbf{k}\rvert$ is the number of classes (in our case, 6). Then, $\mathbf{p}_k(x,y)$ can be interpreted as the probability that pixel $(x,y)$ belongs to class $k$.</p> <h4 id="cross-entropy">Cross-Entropy</h4> <p>The most common loss function for semantic segmentation is pixel-wise cross-entropy, defined as</p> \[\mathcal{L}_{ce} = -\sum_{(x,y)} y(x,y)\,\log p(x,y)\] <p>where $\mathbf{y}(x,y)$ is a one-hot encoded vector of the true class of pixel $(x,y)$</p> <h4 id="weighted-cross-entropy">Weighted Cross-Entropy</h4> <p>Real-world semantic segmentation datasets are often class-imbalanced, leading to issues with basic cross-entropy loss wherein the network is biased toward majority classes. To combat this, class-specific weights are often introduced to the loss function, often derived from the original data statistics <a class="citation" href="#csurka2023semanticimagesegmentationdecades">(Csurka et al., 2023)</a>. Such a cost-sensitive loss function can be seen in the original proposal where the authors introduce a weighting function for each pixel that both balanced the classes and emphasized learning separation borders between cells <a class="citation" href="#ronneberger2015unetconvolutionalnetworksbiomedical">(Ronneberger et al., 2015)</a>. Here, the loss function is of the form</p> \[\mathcal{L}_{wce} = -\sum_{(x,y)} w(x,y)\;y(x,y)\,\log p(x,y)\] <p>where $w$ is a pre-computed function of $(x,y)$.</p> <h4 id="focal-loss">Focal Loss</h4> <p>Focal loss includes a focusing parameter $\gamma$ which controls the down-weighting of well-classified pixels, designed to handle class imbalance and prioritize difficult samples with dice loss for improved segmentation overlap.</p> \[\mathcal{L}_{wfl} = -\sum_{(x,y)}\sum_{c=1}^{\lvert k \rvert} w_c\,(1 - p_c(x,y))^\gamma\,y_c(x,y)\,\log p_c(x,y)\] <h3 id="data-augmentation">Data Augmentation</h3> <p>The image patches are first upsampled to a resolution of $128\times 128$ to ensure they are compatible with the chosen network architecture, which involves a series of convolutional and pooling operations. This particular size is important because the network utilizes skip-connections between the encoder and decoder layers, and it includes five sequential downsampling stages. Each downsampling stage reduces both the height and width of the input by a factor of two, and since $2^5 = 32$, the input dimensions must be multiples of 32 to avoid boundary issues or the need for cropping. By setting the image patches to $128\times 128$, we ensure smooth downsampling at every stage, maintaining feature alignment between the encoder and decoder pathways for effective information transfer.</p> <p>To further enhance the training process and reduce the risk of overfitting, at the start of each training epoch, the original image patch and its associated mask are randomly rotated by a multiple of $90^\circ$. This approach adds rotational invariance to the model’s learned features and expands the effective size of the training dataset, helping the network generalize better to novel samples and preventing it from simply memorizing the training images.</p> <h3 id="model-selection">Model Selection</h3> <p>Pre-trained models from the Segmentation Models PyTorch library <a class="citation" href="#Iakubovskii:2019">(Iakubovskii, 2019)</a> were used. EfficientNet-B0 <a class="citation" href="#tan2020efficientnetrethinkingmodelscaling">(Tan &amp; Le, 2020)</a> pre-trained on ImageNet (missing reference) was chosen as the encoder for its small size (4M parameters), reducing overfitting and accommodating limited compute resources.</p> <h3 id="optimizer--scheduler">Optimizer &amp; Scheduler</h3> <p>The Adam optimizer was used with an initial learning rate of $1\times 10^{-4}$ <a class="citation" href="#Dabra2023">(Dabra &amp; Kumar, 2023)</a>. A LambdaLR scheduler decayed the learning rate by a factor of 0.1 every 40 epochs:</p> \[\lambda(\text{epoch}) = 0.1^{\frac{\text{epoch}}{40}}\] <p>This scheduler systematically reduces the learning rate as training progresses, implementing an exponential decay strategy. Such a decay is beneficial for fine-tuning as it allows the model to make large updates during the initial phases of training when significant adjustments are needed, and smaller, more precise updates in later stages to refine the learned features.</p> <h3 id="early-stopping">Early Stopping</h3> <p>To mitigate the risks of overfitting and the excessive consumption of computational resources during model training, we incorporate an early stopping mechanism in our training pipeline. Early stopping serves as a regularization technique by monitoring the model’s performance on a separate validation dataset and halting the training process when no significant improvement is observed over a predefined number of epochs. Specifically, in my implementation, we track the validation loss at each training epoch and terminate the training if the validation loss does not decrease for five consecutive epochs.</p> <h2 id="results">Results</h2> <h3 id="model-1">Model 1</h3> <p>The initial model follows the Methods exactly and serves as a baseline.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_1_training-480.webp 480w,/assets/img/model_1_training-800.webp 800w,/assets/img/model_1_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_1_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 1 Training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 1 Training </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_1_prediction_grid-480.webp 480w,/assets/img/model_1_prediction_grid-800.webp 800w,/assets/img/model_1_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_1_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 1 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 1 Predictions </div> <h3 id="model-2">Model 2</h3> <p>This model introduces Dice loss alongside cross-entropy to better address class imbalance by directly optimizing overlap between prediction and ground truth.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_2_prediction_grid-480.webp 480w,/assets/img/model_2_prediction_grid-800.webp 800w,/assets/img/model_2_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_2_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 2 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 2 Predictions </div> <h3 id="model-3">Model 3</h3> <p>Class-weighted cross-entropy is combined with Dice loss. Weights were computed as the inverse class frequency and normalized; the unclassified class was scaled down by 0.001 before normalization.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_3_prediction_grid-480.webp 480w,/assets/img/model_3_prediction_grid-800.webp 800w,/assets/img/model_3_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_3_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 3 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 3 Predictions </div> <h3 id="model-4">Model 4</h3> <p>A shallower U-Net (encoder depth 4, decoder channels [256,128,64,32]) reduces compute overhead while maintaining performance.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_4_prediction_grid-480.webp 480w,/assets/img/model_4_prediction_grid-800.webp 800w,/assets/img/model_4_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_4_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 4 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 4 Predictions </div> <h3 id="model-5">Model 5</h3> <p>Dropout (p=0.2) was added in the decoder to improve generalization by reducing overfitting.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_5_prediction_grid-480.webp 480w,/assets/img/model_5_prediction_grid-800.webp 800w,/assets/img/model_5_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_5_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 5 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 5 Predictions </div> <h3 id="model-6">Model 6</h3> <p>SCSE attention blocks were incorporated in the decoder to recalibrate feature channels and focus on important regions <a class="citation" href="#roy2018recalibratingfullyconvolutionalnetworks">(Roy et al., 2018)</a>. SCSE blocks recalibrate feature responses by adaptively weighting each channel through a squeeze operation (global average pooling) followed by an excitation step using fully connected layers and sigmoid activation. This mechanism allows the network to emphasize important features while suppressing irrelevant ones, enhancing the model’s focus on key regions of the feature maps.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_6_prediction_grid-480.webp 480w,/assets/img/model_6_prediction_grid-800.webp 800w,/assets/img/model_6_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_6_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 6 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 6 Predictions </div> <h3 id="model-7">Model 7</h3> <p>Combines SCSE attention, weighted focal loss, Dice loss, AdamW optimizer, and a cosine annealing scheduler with warm restarts for refined performance.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_7_training-480.webp 480w,/assets/img/model_7_training-800.webp 800w,/assets/img/model_7_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_7_training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 7 Training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 7 Training </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_7_prediction_grid-480.webp 480w,/assets/img/model_7_prediction_grid-800.webp 800w,/assets/img/model_7_prediction_grid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_7_prediction_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model 7 Predictions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Model 7 Predictions </div> <h2 id="model-comparison">Model Comparison</h2> <p>Model 1 stands out as the best-performing model overall. It consistently achieves the highest Dice and IoU scores for critical classes, while maintaining strong performance across other classes. Model 4 closely follows, excelling particularly in Class 1 and Class 3, where it outperforms other models. Model 6 also demonstrates strong performance, particularly in Class 3 and Class 5, where its inclusion of SCSE attention blocks helps improve feature focus and segmentation quality. While it falls slightly behind Models 1 and 4 in certain classes, it remains a reliable model with strong average Dice and IoU scores. On the other hand, Model 2 and Model 7 underperform.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_comparison_1-480.webp 480w,/assets/img/model_comparison_1-800.webp 800w,/assets/img/model_comparison_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_comparison_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Overall Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall Comparison </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_comparison_2-480.webp 480w,/assets/img/model_comparison_2-800.webp 800w,/assets/img/model_comparison_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_comparison_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Class-Wise IoU Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Class-Wise IoU Comparison </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_comparision_3-480.webp 480w,/assets/img/model_comparision_3-800.webp 800w,/assets/img/model_comparision_3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/model_comparision_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Class-Wise Dice Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Class-Wise Dice Comparison </div> <h2 id="conclusion">Conclusion</h2> <p>This work highlights the strengths and trade-offs of various modifications to the U-Net architecture for aerial image segmentation. Model 1, despite being the most basic, was the highest-performing model across all evaluation metrics. Model 4 demonstrates that computational efficiency can be achieved without substantial loss in accuracy. Model 6 showcases the value of attention mechanisms for complex regions. However, limitations remain in capturing fine boundaries and in class imbalance. A simple improvement may be to use an ensemble of U-Nets <a class="citation" href="#Marmanis2016">(Marmanis et al., 2016)</a>.</p> <h2 id="future-work">Future Work</h2> <p>While this project showed promising results, improvements can certainly be made. The models seemed to struggle with fine-detailed class boundaries, for example, a jagged settlement bordering a patch of vegetation. To enhance the precision of these boundaries, future work could explore incorporating higher-resolution input data, which would provide more detailed information for the model to learn from. Additionally, implementing multi-scale feature extraction techniques, such as feature pyramids, could help the model capture both global and local context more effectively. Another promising approach is the integration of Conditional Random Fields as a post-processing step to refine segmentation edges by considering spatial dependencies and contextual relationships between pixels. Employing these methods could lead to smoother and more accurate delineations of classes with intricate boundaries and improve overall segmentation performance.</p> <p>Class imbalance proved to be an issue throughout the project, with classes like water dominating the optimization problem. While class-balancing was attempted through loss function weighting, more methods exist to address this challenge. Future work could investigate advanced strategies such as Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples for underrepresented classes, thereby increasing their presence in the training dataset. Another potential approach is the use of data augmentation techniques such as geometric transformations or color jittering to artificially enhance the diversity and quantity of these classes. Exploring these methods could lead to a more balanced training process and improve the model’s ability to accurately segment all classes.</p> <p>I chose a relatively basic encoder given time and compute constraints, but more advanced encoders such as ResNet or Vision Transformers (ViT) could be integrated to enhance feature extraction capabilities. Vision Transformers, which leverage self-attention mechanisms to capture long-range dependencies within the image, might enable the model to better understand complex spatial relationships, leading to more accurate semantic segmentation.</p> <p>Finally, it would be useful to explore other state-of-the-art models for semantic segmentation beyond the U-Net. Architectures such as DeepLabv3+, PSPNet, and Mask R-CNN offer alternative approaches that incorporate advanced techniques like atrous convolutions, pyramid pooling modules, and instance segmentation capabilities. For example, DeepLabv3+ utilizes atrous spatial pyramid pooling to capture multi-scale contextual information, which can improve the segmentation of objects at different sizes. PSPNet’s pyramid pooling module effectively aggregates global and local context, enhancing the model’s ability to understand complex scenes. Mask R-CNN extends the capabilities of object detection frameworks to perform instance segmentation, allowing for more precise delineation of objects within an image.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="NASM_AerialPhotography" class="col-sm-8"> <div class="title">The Beginnings and Basics of Aerial Photography</div> <div class="author"> National Air and Space Museum </div> <div class="periodical"> Jul 2023 </div> <div class="periodical"> Accessed: Dec. 8, 2024 </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zhang2023dive" class="col-sm-8"> <div class="title">Dive into Deep Learning</div> <div class="author"> Aston Zhang, Zachary C. Lipton, Mu Li, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alexander J. Smola' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jul 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Dabra2023" class="col-sm-8"> <div class="title">Evaluating green cover and open spaces in informal settlements of Mumbai using deep learning</div> <div class="author"> Ayush Dabra and Vaibhav Kumar </div> <div class="periodical"> <em>Neural Computing and Applications</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1007/s00521-023-08320-7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="csurka2023semanticimagesegmentationdecades" class="col-sm-8"> <div class="title">Semantic Image Segmentation: Two Decades of Research</div> <div class="author"> Gabriela Csurka, Riccardo Volpi, and Boris Chidlovskii </div> <div class="periodical"> Jul 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="tan2020efficientnetrethinkingmodelscaling" class="col-sm-8"> <div class="title">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</div> <div class="author"> Mingxing Tan and Quoc V. Le </div> <div class="periodical"> Jul 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Iakubovskii:2019" class="col-sm-8"> <div class="title">Segmentation Models Pytorch</div> <div class="author"> Pavel Iakubovskii </div> <div class="periodical"> Jul 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="roy2018recalibratingfullyconvolutionalnetworks" class="col-sm-8"> <div class="title">Recalibrating Fully Convolutional Networks with Spatial and Channel ’Squeeze &amp; Excitation’ Blocks</div> <div class="author"> Abhijit Guha Roy, Nassir Navab, and Christian Wachinger </div> <div class="periodical"> Jul 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Marmanis2016" class="col-sm-8"> <div class="title">Semantic Segmentation of Aerial Images with an Ensemble of CNNs</div> <div class="author"> D. Marmanis, J. D. Wegner, S. Galliani, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'K. Schindler, M. Datcu, U. Stilla' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences</em>, Jul 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.5194/isprs-annals-III-3-473-2016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ronneberger2015unetconvolutionalnetworksbiomedical" class="col-sm-8"> <div class="title">U-Net: Convolutional Networks for Biomedical Image Segmentation</div> <div class="author"> Olaf Ronneberger, Philipp Fischer, and Thomas Brox </div> <div class="periodical"> Jul 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Baumann_RemoteSensingHistory" class="col-sm-8"> <div class="title">HISTORY OF REMOTE SENSING, AERIAL PHOTOGRAPHY</div> <div class="author"> P. Baumann </div> <div class="periodical"> Jul 2014 </div> <div class="periodical"> Accessed: Dec. 8, 2024 </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Grayson M. Martin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>