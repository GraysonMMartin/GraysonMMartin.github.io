<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RL for High-Performance Jumping | Grayson M. Martin </title> <meta name="author" content="Grayson M. Martin"> <meta name="description" content="Used a curriculum learning framework to teach simulation quadrupeds to jump. Worked on and written with Aryan Naveen and Pranay Varada."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://graysonmmartin.github.io/projects/cs1840_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Grayson</span> M. Martin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RL for High-Performance Jumping</h1> <p class="post-description">Used a curriculum learning framework to teach simulation quadrupeds to jump. Worked on and written with Aryan Naveen and Pranay Varada.</p> </header> <article> <h2 id="abstract">Abstract</h2> <p>Agents acting in dynamic environments must be able to adapt to the conditions of these environments. Quadrupedal robots are particularly useful for simulating agents in these environments because of their flexibility and real‑world applicability. This reinforcement learning project aims to teach a quadruped to jump over a moving obstacle, using Proximal Policy Optimization (PPO) and a two‑stage curriculum learning framework. Stage I focuses on achieving the ideal jumping motion, while Stage II integrates dynamic obstacles, utilizing a Markov Decision Process (MDP) modified to include motion dynamics and obstacle detection. Reference state initialization (RSI) and domain randomization are employed to enhance robustness and generalization. Simulations in IsaacGym demonstrated that two‑stage curriculum learning improved upon direct training in minimizing obstacle collisions, particularly for obstacles at closer ranges. Timing remains a challenge when attempting to avoid obstacles initialized from farther away. This work highlights the value of curriculum learning methods in RL for robotic tasks and proposes future improvements in reward shaping and learning strategy for enhanced adaptability in dynamic environments.</p> <hr> <h2 id="1introduction">1 Introduction</h2> <p>Throughout the field of reinforcement learning, there is significant demand for creating strategies to learn an optimal policy that can achieve a desired reward across changing environments. Furthermore, it is essential that such a policy can <em>understand</em> the changes in these environments and act accordingly. After all, many environments in the real world are non‑stationary, with varying conditions in weather and terrain acting on agents, for example. Adaptive policies are also suitable for long‑term success because of their higher levels of robustness. In use cases from search‑and‑rescue operations to industrial inspections, such adaptability can be critical.</p> <p>Our project aims to solve this problem of understanding changing environments through teaching a quadruped to react to an obstacle moving towards it with a random angle and velocity. Quadrupedal robots such as Boston Dynamics’ Spot and ANYbotics’ ANYmal are particularly advantageous for undertaking such a project for three reasons. First, there is extensive literature on reinforcement learning for quadruped locomotion, which means that we can iterate on different implementations of RL strategies in order to solve new problems. Specifically, while quadrupedal jumping may be well studied, our project aims to understand how an agent can react to random timing and position. Second, simulations of quadruped movement are both high‑dimensional and visually easy to comprehend, making it clear what the agent has learned once the training process is complete. Third, there is a strong basis for quadruped simulation and translation to real‑world situations where changing environments may be at play; a desire to understand environments motivated our decision to work on this project.</p> <p>To tackle the particular scenario of jumping over a moving obstacle, we consider several RL techniques. We begin with Proximal Policy Optimization (PPO) as the foundational approach, and subsequently integrate curriculum learning strategies into the training process to systematically optimize the quadruped’s ability to master this (perhaps surprisingly) complex task. Curriculum learning breaks this task down into sub‑problems, such that the quadruped first learns how to jump, and once it has mastered that skill, learns how to jump over a moving obstacle.</p> <p>The remainder of the paper is structured as follows: Sections 2 and 3 detail the existing theory and literature that are pertinent to the experiments we carried out in this report; Section 4 details the modified MDP for our desired problem setting and task of jumping over dynamic obstacles. Sections 5 and 6 detail the selected approaches to solving this problem based on the pre‑existing literature. Finally, in Section 7 we detail the simulated results we observed and evaluate the learned behavior’s performance for various approaches.</p> <hr> <h2 id="2preliminaries">2 Preliminaries</h2> <p>Proximal Policy Optimization (PPO) is a policy‑gradient method that improves upon previous methods in the literature through its relative ease of implementation—requiring just first‑order optimization—and greater robustness relative to optimization techniques such as Trust Region Policy Optimization (TRPO) <a class="citation" href="#schulman2017proximalpolicyoptimizationalgorithms">(Schulman et al., 2017)</a>. While the objective function maximized by TRPO is the expectation of the product of the advantage and a probability ratio measuring the change between the new and old policy at an update, PPO’s objective function <strong>clips</strong> the probability ratio in this surrogate objective in order to prevent the policy from making unstable updates while simultaneously continuing to allow for exploration. Clipping also avoids having to constrain the KL divergence, making the process computationally simpler and enabling policy updates over multiple epochs. PPO’s simplicity and stability make it a commonplace strategy for finding the optimal policy in RL, which is why we use it as a baseline from which we search for possible improvements, namely curriculum learning methods.</p> <hr> <h2 id="3literature-review">3 Literature Review</h2> <p>Atanassov, Ding, Kober, Havoutis, and Santina use curriculum learning to stratify the problem of quadrupedal jumping into different sub‑tasks, in order to demonstrate that reference trajectories of mastered jumping are not necessary for learning the optimal policy in this scenario <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a>. This increases the adaptability of such a policy, because it is learned by the robot on its own, enabling it to generalize better to unseen real‑world scenarios. Another important component of achieving the optimal policy is reward shaping, which Kim, Kwon, Kim, Lee, and Oh tackle in a stage‑wise fashion in the context of a humanoid backflip <a class="citation" href="#kim2024stagewiserewardshapingacrobatic">(Kim et al., 2024)</a>. By developing customized reward and cost definitions for each element of a successful backflip, a complex maneuver like this is segmented into an intuitive fashion that translates well to real‑world dynamics.</p> <hr> <h2 id="4problem-formulation">4 Problem Formulation</h2> <p>We utilize a Markov Decision Process (MDP) as the underlying sequential decision‑making model for our RL problem. The MDP is described as a tuple<br> \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, r, P, \rho, \gamma)\)<br> where:</p> <ul> <li>$\mathcal{S}$ is the state space</li> <li>$\mathcal{A}$ is the action space</li> <li>$r: \mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the reward function</li> <li>$P: \mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})$ is the transition operator</li> <li>$\rho\in\Delta(\mathcal{S})$ is the initial distribution</li> <li>$\gamma\in(0,1)$ is the discount factor</li> </ul> <p>The overall objective of reinforcement learning is to find a policy $\pi:\mathcal{S}\to\mathcal{A}$ that maximizes the cumulative infinite‑horizon discounted reward:<br> \(\mathbb{E}_{s_0\sim\rho,\pi}\Bigl[\sum_{i=0}^\infty \gamma^i\,r(s_i,a_i)\,\Bigm|\,s_0\Bigr].\)<br> A given policy $\pi$ has a value function under transition dynamics $P$ defined as<br> \(V^\pi_P(s)=\mathbb{E}_\pi\Bigl[\sum_{i=0}^\infty \gamma^i\,r(s_i,a_i)\mid s_0=s\Bigr],\)<br> and the state‑action value function is similarly defined as<br> \(Q^\pi_P(s,a)=\mathbb{E}_\pi\Bigl[\sum_{i=0}^\infty \gamma^i\,r(s_i,a_i)\mid s_0=s,\;a_0=a\Bigr].\)</p> <h3 id="41quadruped-jumping-obstacle-avoidance-mdp">4.1 Quadruped Jumping Obstacle Avoidance MDP</h3> <p>As outlined in Section 1, in this report we extend the work of <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a> to enable a quadruped to jump over dynamic obstacles. This enhancement requires not only integrating curriculum learning, but also modifying the underlying MDP formulation to account for the quadruped’s perception module’s feedback as shown in Figure 1.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/quad_controller-480.webp 480w,/assets/img/quad_controller-800.webp 800w,/assets/img/quad_controller-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/quad_controller.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Hierarchical control framework for a quadruped robot" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of the hierarchical control framework for a quadruped robot. The trained policy $\pi$ yields desired joint position deviations from the nominal joint positions that are then fed into a low‑level PD controller producing necessary torques $\tau$ for each joint. </div> <h4 id="state-space">State Space</h4> <p>Building on <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a>, we leverage a memory of previous observations and actions to enable the agent to implicitly reason about its own dynamics. We concatenate over a window of $N$ timesteps:</p> <ul> <li>Base linear velocity $\mathbf{v}\in\mathbb{R}^{3\times N}$</li> <li>Base angular velocity $\boldsymbol{\omega}\in\mathbb{R}^{3\times N}$</li> <li>Joint positions $\mathbf{q}\in\mathbb{R}^{12\times N}$ and velocities $\dot{\mathbf{q}}\in\mathbb{R}^{12\times N}$</li> <li>Previous actions $\mathbf{a}_{t-1}\in\mathbb{R}^{12\times N}$</li> <li>Base orientation $\bar{q}\in\mathbb{R}^{4\times N}$</li> <li>Foot contact states $\mathbf{c}\in\mathbb{R}^{4\times N}$</li> </ul> <p>To handle dynamic obstacles, we add obstacle detection flags $i^\zeta$ and obstacle endpoints $\zeta$—each also tracked over $N$ frames—resulting in a total dimension of $60N$.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/state_space-480.webp 480w,/assets/img/state_space-800.webp 800w,/assets/img/state_space-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/state_space.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="State representation for jumping over dynamic obstacles" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of the state representation used for training our jumping over dynamic obstacle policies. The state vector $s_t\in\mathbb{R}^{60N}$ is constructed by concatenating perception states (obstacle detection and endpoints) and robot states over the past $N$ timesteps. </div> <h4 id="action-space">Action Space</h4> <p>As is standard, the policy outputs deviations $\Delta\mathbf{q}\in\mathbb{R}^{12}$ from nominal joint positions $\mathbf{q}^{\mathrm{nom}}\in\mathbb{R}^{12}$, which are filtered, scaled, and then passed to a low‑level PD controller.</p> <h4 id="reward">Reward</h4> <p>Inspired by <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a>, we define \(r_{\mathrm{total}} = r^+ \exp\!\Bigl(-\|\;r^-\;\|^2/\sigma\Bigr)^4,\) where positive components $r^+$ are multiplied by an exponential penalty on the negative components $r^-$.</p> <ul> <li> <strong>Dense rewards:</strong> tracking flight velocity, squat height, foot clearance, and penalizing energy use.</li> <li> <strong>Sparse rewards:</strong> episode‑level bonuses for successful jumps and penalties for fall‑over, collision, or large orientation errors.</li> </ul> <hr> <h2 id="5state-initialization--domain-randomization">5 State Initialization &amp; Domain Randomization</h2> <p>Static start states can hinder exploration. Peng <em>et al.</em>’s Reference State Initialization (RSI) <a class="citation" href="#Peng_2018">(Peng et al., 2018)</a> samples initial states from an expert trajectory. In our work we use a modified RSI: for Stage I we uniformly sample height and $z$‑velocity as in Table 1.</p> <table> <thead> <tr> <th>State Variable</th> <th>Min</th> <th>Max</th> </tr> </thead> <tbody> <tr> <td>Height (m)</td> <td>0</td> <td>0.3</td> </tr> <tr> <td>$z$‐velocity (m/s)</td> <td>-0.5</td> <td>3</td> </tr> </tbody> </table> <p><em>Table 1: Initialization ranges for Stage I</em></p> <p>Stage II adds obstacle randomization (Table 2):</p> <table> <thead> <tr> <th>State Variable</th> <th>Min</th> <th>Max</th> </tr> </thead> <tbody> <tr> <td>Obstacle distance $r$ (m)</td> <td>3</td> <td>7</td> </tr> <tr> <td>Obstacle direction $\theta$ (rad)</td> <td>0</td> <td>$2\pi$</td> </tr> <tr> <td>Obstacle orientation $\gamma$ (rad)</td> <td>$\frac{\pi}{2}-\theta-\frac{\pi}{3}$</td> <td>$\frac{\pi}{2}-\theta+\frac{\pi}{3}$</td> </tr> <tr> <td>Obstacle velocity (m/s)</td> <td>3.5</td> <td>7</td> </tr> </tbody> </table> <p><em>Table 2: Initialization ranges for Stage II</em></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/obstacle_init-480.webp 480w,/assets/img/obstacle_init-800.webp 800w,/assets/img/obstacle_init-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/obstacle_init.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Obstacle initialization properties" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> State‑space variables related to the obstacle. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rsi_plot-480.webp 480w,/assets/img/rsi_plot-800.webp 800w,/assets/img/rsi_plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rsi_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Stage I training with and without RSI" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Stage I training performance, demonstrating the impact of RSI. </div> <p>Domain randomization—varying friction, masses, latencies, etc.—further improves sim‑to‑real generalization <a class="citation" href="#tobin2017domainrandomizationtransferringdeep">(Tobin et al., 2017)</a> and was adopted from <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a>.</p> <hr> <h2 id="6obstacle-avoidance-curriculum-learning">6 Obstacle Avoidance Curriculum Learning</h2> <p>Curriculum learning [5, 6] presents tasks in increasing order of difficulty. In quadruped jumping <a class="citation" href="#atanassov2024curriculumbasedreinforcementlearningquadrupedal">(Atanassov et al., 2024)</a>, Stage I teaches jumps in place, Stage II teaches positional jumps, and Stage III jumps onto platforms. We adapt this to:</p> <ol> <li> <strong>Stage I (5 k iters):</strong> learn to jump in place (no obstacle).</li> <li> <strong>Stage II (15 k iters):</strong> introduce flying obstacle, add collision penalty<br> \(r_{\mathrm{col}} = \mathbbm{1}\bigl\{\min_{f\in\text{feet}}\|\mathbf{f}-\zeta\|\le0.1\bigr\}.\)</li> <li> <strong>No revisit to Stage I</strong>—we reduce the squat reward scale from 5 to 1 to prioritize obstacle avoidance over ideal form.</li> </ol> <hr> <h2 id="7experiments">7 Experiments</h2> <h3 id="71training-environment">7.1 Training Environment</h3> <p>We use NVIDIA IsaacGym for massive parallelism—thousands of simulenvs on one GPU—and seamless PyTorch integration.</p> <h3 id="72network-architecture">7.2 Network Architecture</h3> <p>Actor and critic both: FC layers of sizes $|s|$–512–512–(12 or 1), ReLU activations, $\tanh$‑normalized outputs.</p> <h3 id="73results">7.3 Results</h3> <p>We evaluated collision counts (out of 50) for obstacles initialized at distances 3–7 m. Curriculum learning outperforms direct training at all distances, especially at 3 m.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/collisions_plot-480.webp 480w,/assets/img/collisions_plot-800.webp 800w,/assets/img/collisions_plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/collisions_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Number of collisions at a given distance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Number of collisions at each initialization distance (50 trials). </div> <p>The quadruped learned the jump but often jumped too early at larger radii, landing before the obstacle arrived (Figure 6). Future work should encourage timed jumps.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/jump-480.webp 480w,/assets/img/jump-800.webp 800w,/assets/img/jump-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/jump.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Quadruped jumping over a flying obstacle" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The quadruped clears a flying obstacle after training. </div> <hr> <h2 id="8conclusion">8 Conclusion</h2> <p>We demonstrated that PPO plus a two‑stage curriculum and modified RSI enable a quadruped to jump over moving obstacles. Curriculum learning accelerates skill acquisition and improves obstacle avoidance. Modified RSI broadens training states, and domain randomization aids sim‑to‑real transfer. Future directions include:</p> <ul> <li>Teaching the agent to jump <em>only</em> when collision is imminent.</li> <li>Multiple successive jumps (jump‑roping).</li> <li>Expert trajectories (true RSI) or imitation learning (DAgger).</li> <li>Predictive modules for obstacle trajectories.</li> </ul> <hr> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="atanassov2024curriculumbasedreinforcementlearningquadrupedal" class="col-sm-8"> <div class="title">Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-free Design</div> <div class="author"> Vassil Atanassov, Jiatao Ding, Jens Kober, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ioannis Havoutis, Cosimo Della Santina' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="kim2024stagewiserewardshapingacrobatic" class="col-sm-8"> <div class="title">Stage-Wise Reward Shaping for Acrobatic Robots: A Constrained Multi-Objective Reinforcement Learning Approach</div> <div class="author"> Dohyeong Kim, Hyeokjin Kwon, Junseok Kim, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Gunmin Lee, Songhwai Oh' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Peng_2018" class="col-sm-8"> <div class="title">DeepMimic: example-guided deep reinforcement learning of physics-based character skills</div> <div class="author"> Xue Bin Peng, Pieter Abbeel, Sergey Levine, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Michiel Panne' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM Transactions on Graphics</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3197517.3201311" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="schulman2017proximalpolicyoptimizationalgorithms" class="col-sm-8"> <div class="title">Proximal Policy Optimization Algorithms</div> <div class="author"> John Schulman, Filip Wolski, Prafulla Dhariwal, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Alec Radford, Oleg Klimov' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> Jul 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="tobin2017domainrandomizationtransferringdeep" class="col-sm-8"> <div class="title">Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</div> <div class="author"> Josh Tobin, Rachel Fong, Alex Ray, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jonas Schneider, Wojciech Zaremba, Pieter Abbeel' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jul 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Grayson M. Martin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>